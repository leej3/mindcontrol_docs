{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import os.path as op\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from IPython.display import Image\n",
    "from nipype import MapNode, Workflow, Node\n",
    "from nipype.interfaces.freesurfer import MRIConvert\n",
    "from nipype.interfaces.io import DataSink\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get intersection of niftis, metadata, and freesurfer, and non-callibration output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niftis_dir = Path('/data/Dnude/bids_work/nifti_converted_images')\n",
    "archive_exts = ['.nii.gz','.tgz']\n",
    "bids_info_pklz = Path('bids_info.pklz')\n",
    "\n",
    "df_nii = pd.DataFrame(\n",
    "    {'nifti_path' :\n",
    "     [p.as_posix() for p in niftis_dir.glob('**/*.nii.gz')]}\n",
    ")\n",
    "\n",
    "df_nii.nifti_path[0]\n",
    "\n",
    "exts_regex = '(?:' + '|'.join(archive_exts) + ')'\n",
    "\n",
    "df_nii = pd.concat([df_nii,\n",
    "                       df_nii.nifti_path.str.extract(\n",
    "                              '.*/(?P<mask_id>[0-9]{2,5})/.*' + exts_regex,expand=True)],\n",
    "                       axis = 1)\n",
    "\n",
    "# Filter original ge4x\n",
    "\n",
    "df_nii = df_nii.loc[~df_nii.nifti_path.str.contains('orig'),:]\n",
    "df_nii.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_nii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv('/data/Dnude/bids_work/metadata/AdamT_NV_data_exchange_Batch01.csv')\n",
    "df_metadata = df_metadata.assign(\n",
    "    ScanID=lambda df:df.ScanID.astype(int).astype(str))\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_nii.merge(df_metadata,how = 'outer',indicator=True, left_on= 'mask_id', right_on= 'ScanID' )\n",
    "\n",
    "df_merge.query(\"_merge != 'both'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_filtered = df_merge.query(\"_merge == 'both'\").drop(labels = '_merge',axis = 1)\n",
    "\n",
    "len(df_merge_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freesurfer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs = pd.DataFrame({'fs_id_path' : [f.as_posix() for f in Path('/data/Dnude/subjects').glob('*')]})\n",
    "\n",
    "df_fs = pd.concat([df_fs,\n",
    "                       df_fs.fs_id_path.str.extract(\n",
    "                              '.*/(?P<fs_id>[0-9]{2,5})',expand=True)],\n",
    "                       axis = 1).assign(\n",
    "mask_id = lambda df:[str(int(x.split('/')[-1])) for x in df.fs_id])\n",
    "df_fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_fs.merge(df_merge_filtered,on = 'mask_id',how = 'outer', indicator = True)\n",
    "len(df_full.query(\"_merge != 'both'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_filtered = (df_full.\n",
    "                    query(\"_merge == 'both'\").\n",
    "                    drop(labels = '_merge',axis = 1)\n",
    "                   )\n",
    "df_full_filtered = df_full_filtered.loc[~df_full_filtered.CalibrationStudies.str.contains('not for public release'),:]\n",
    "df_full_filtered = df_full_filtered.sort_values(['MRN','DateScanText','mask_id'])\n",
    "len(df_full_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate bids mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_key_path = Path('sub_key.pklz')\n",
    "def add_bids_ses(df):\n",
    "    df = df.assign(bids_ses = ['{num:05d}'.format(num = 1 + i) for i in range(len(df))])\n",
    "    return df\n",
    "\n",
    "generate_keys = True\n",
    "if generate_keys:\n",
    "    def gen_bids_subj(row,sub_key_path,id_col = 'patient_id'):\n",
    "        if sub_key_path.exists():\n",
    "            sub_dict = pd.read_pickle(sub_key_path)\n",
    "            sub = sub_dict.get(row[id_col],None)\n",
    "            if not sub:\n",
    "                sub_dict[row[id_col]] = '{n:04d}'.format(n = 1 + max([int(s) for s in sub_dict.values]))\n",
    "        else:\n",
    "            sub_dict = pd.Series()\n",
    "            sub = '0001'\n",
    "            sub_dict[row[id_col]] = sub\n",
    "            \n",
    "        \n",
    "        \n",
    "        pd.to_pickle(sub_dict,path=sub_key_path)\n",
    "\n",
    "        row['bids_subj'] = sub\n",
    "        return row    \n",
    "else:\n",
    "    def gen_bids_subj(row,sub_key_path):\n",
    "        sub_dict = pd.read_csv('test',sep=' ',dtype=str,header=None,names = ['mrn','sub_id'])\n",
    "        mrn = row.sub_id\n",
    "        sub = sub_dict.query('mrn ==@mrn').sub_id.values[0]\n",
    "        if not sub:\n",
    "            raise ValueError('sub key does not exist')\n",
    "        row['bids_subj'] = sub\n",
    "        return row    \n",
    "\n",
    "df_bids = df_full_filtered\n",
    "df_bids = df_bids.apply(lambda row: gen_bids_subj(row,sub_key_path,'MRN'),axis = 1)\n",
    "df_bids['bids_ses'] = df_bids.fs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate symlinked freesurfer_dir with bids mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linked_fs_dir = Path('linked_bids_2018_08_14') Incorrect\n",
    "linked_fs_dir = Path('linked_freesurfer_maskid')\n",
    "linked_fs_dir.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not linked_fs_dir.exists():\n",
    "    linked_fs_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_bids.itertuples():\n",
    "    sub_ses = 'sub-' + row.bids_subj + '_ses-' + row.bids_ses\n",
    "    !cd {linked_fs_dir.absolute()};ln -s {row.fs_id_path} {sub_ses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Mindcontrol to QC Freesurfer Outputs\n",
    "\n",
    "This tutorial will set up a Mindcontrol instance for Freesurfer output QC. The first step is to clone the Mindcontrol repository from GitHub and checkout the FreeSurfer branch:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/akeshavan/mindcontrol\n",
    "cd mindcontrol\n",
    "git checkout origin/FreeSurfer\n",
    "```\n",
    "\n",
    "Then start Mindcontrol by typing `meteor` in the mindcontrol directory:\n",
    "\n",
    "```bash\n",
    "meteor\n",
    "```\n",
    "\n",
    "Also, start the static server in another terminal window:\n",
    "\n",
    "```bash\n",
    "cd mindcontrol_docs/mindcontrol_base_dir\n",
    "python start_static_server.py\n",
    "```\n",
    "\n",
    "Navigate to http://localhost:3000, you should see an empty Mindcontrol UI:\n",
    "\n",
    "![](images/freesurfer_empty_instance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import FreeSurfer Subjects Directory Into Mindcontrol\n",
    "\n",
    "Freesurfer writes data in `.mgz` format, but Mindcontrol's Papaya.js viewer can only read `.nii.gz` files. We also want to split up the different parts of FreeSurfer QC (brain mask editing, wm edits, and aparc+aseg edits) into 3 sections, as shown in the empty Mindcontrol instance above. This workflow reads all the folders in the `SUBJECTS_DIR` of Freesurfer, converts all volumetric data from `.mgz` into `.nii.gz`, and parses stats files to display as histograms on the mindcontrol interface.\n",
    "\n",
    "Lets say your directory structure looks like:\n",
    "\n",
    "```bash\n",
    "mindcontrol/ #(cloned from https://github.com/akeshavan/mindcontrol, checked out origin/FreeSurfer branch)\n",
    "\n",
    "mindcontrol_docs/ #(cloned from https://github.com/akeshavan/mindcontrol_docs)\n",
    "|___ MindPrepFS.ipynb #(this notebook)\n",
    "|___ mindcontrol_base_dir/\n",
    "     |___ start_static_server.py\n",
    "\n",
    "freesurfer_subjects_dir/\n",
    "|___ sub-01/\n",
    "    |___ mri/\n",
    "    |___ stats/\n",
    "    |___ surf/\n",
    "     ...\n",
    " ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages\n",
    "\n",
    "We recommend installing nipype through [Anaconda](https://www.continuum.io/downloads):\n",
    "\n",
    "```bash\n",
    "conda config --add channels conda-forge\n",
    "conda install -y nipype\n",
    "```\n",
    "\n",
    "Also make sure you have [FreeSurfer](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=freesurfer+download&*) installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run conversion for mindcontrol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SUBJECTS_DIR, from Freesurfer\n",
    "subjects_dir = linked_fs_dir.absolute()\n",
    "\n",
    "# Set the directory to save converted Freesurfer outputs (ideally, the mindcontrol base dir)\n",
    "mindcontrol_base_dir = os.path.abspath(\"./mindcontrol_base_dir\") #this is where the static_server runs\n",
    "mindcontrol_outdir = os.path.abspath(\"./mindcontrol_base_dir/freesurfer\") \n",
    "if not os.path.exists(mindcontrol_outdir):\n",
    "    os.mkdir(mindcontrol_outdir)\n",
    "\n",
    "# Set the directory where the nipype workflow saves intermediary files\n",
    "workflow_working_dir = os.path.abspath(\"/lscratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this worflow, your directory structure will look like:\n",
    "\n",
    "```bash\n",
    "mindcontrol/ \n",
    "# you should be running \"meteor\" in a separate terminal window here\n",
    "\n",
    "mindcontrol_docs/ \n",
    "|___ MindPrepFS.ipynb \n",
    "|___ mindcontrol_base_dir/\n",
    "     |___ start_static_server.py\n",
    "          # you should be running start_static_server.py in a separate terminal window\n",
    "     |___ freesurfer/ #the workflow will create these files\n",
    "         |___ sub-01/\n",
    "             |___ T1.nii.gz\n",
    "             |___ aparc+aseg.nii.gz\n",
    "             |___ brainmask.nii.gz\n",
    "             |___ wm.nii.gz\n",
    "             |___ mindcontrol_entries.json\n",
    "|___ scratch/ #this is the nipype working directory\n",
    "\n",
    "freesurfer_subjects_dir/\n",
    "|___ sub-01/\n",
    "    |___ mri/\n",
    "    |___ stats/\n",
    "    |___ surf/\n",
    "     ...\n",
    " ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all subjects in the Freesurfer directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_paths = glob(op.join(subjects_dir, \"*\"))\n",
    "# SUBJECTS_DIR sometimes contains directories that aren't subjects, don't grab them\n",
    "subjects = []\n",
    "for path in subject_paths:\n",
    "    subject = path.split('/')[-1]\n",
    "    # check if mri dir exists, and don't add fsaverage\n",
    "    if op.exists(op.join(path, 'mri')) and subject != 'fsaverage':\n",
    "        subjects.append(subject)\n",
    "        # Volumes that will be prepared\n",
    "volumes = [\"brainmask.mgz\", \"wm.mgz\", \"aparc+aseg.mgz\", \"T1.mgz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mriqc = pd.read_csv('/data/Dnude/bids_work/synced_analysis_directory/anatMRIQC.csv')\n",
    "# df_armin = pd.read_csv('/data/Dnude/bids_work/metadata/Armin_Science_1531_MASKIDs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function finds data in the subjects_dir \n",
    "def data_grabber(subjects_dir, subject, volumes):\n",
    "    import os\n",
    "    volumes_list = [os.path.join(subjects_dir, subject, 'mri', volume) for volume in volumes]\n",
    "    return volumes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function parses the aseg.stats, lh.aparc.stats and rh.aparc.stats and returns a dictionary\n",
    "\n",
    "def parse_stats(subjects_dir, subject):\n",
    "    from os.path import join, exists\n",
    "\n",
    "    aseg_file = join(subjects_dir, subject, \"stats\", \"aseg.stats\")\n",
    "    lh_aparc = join(subjects_dir, subject, \"stats\", \"lh.aparc.stats\")\n",
    "    rh_aparc = join(subjects_dir, subject, \"stats\", \"rh.aparc.stats\")\n",
    "\n",
    "    assert exists(aseg_file), \"aseg file does not exists for %s\" %subject\n",
    "    assert exists(lh_aparc), \"lh aparc file does not exists for %s\" %subject\n",
    "    assert exists(rh_aparc), \"rh aparc file does not exists for %s\" %subject\n",
    "\n",
    "    def convert_stats_to_json(aseg_file, lh_aparc, rh_aparc):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        def extract_other_vals_from_aseg(f):\n",
    "            value_labels = [\"EstimatedTotalIntraCranialVol\",\n",
    "                              \"Mask\",\n",
    "                              \"TotalGray\",\n",
    "                              \"SubCortGray\",\n",
    "                              \"Cortex\",\n",
    "                              \"CerebralWhiteMatter\",\n",
    "                              \"CorticalWhiteMatterVol\",\n",
    "                           \"SurfaceHoles\"]\n",
    "            value_labels = list(map(lambda x: 'Measure ' + x + ',', value_labels))\n",
    "            output = pd.DataFrame()\n",
    "            with open(f,\"r\") as q:\n",
    "                out = q.readlines()\n",
    "                relevant_entries = [x for x in out if any(v in x for v in value_labels)]\n",
    "                for val in relevant_entries:\n",
    "                    sname= val.split(\",\")[1][1:]\n",
    "                    vol = val.split(\",\")[-2]\n",
    "                    output = output.append(pd.Series({\"StructName\":sname,\"Volume_mm3\":vol}),ignore_index=True)\n",
    "            return output\n",
    "\n",
    "        df = pd.DataFrame(np.genfromtxt(aseg_file,dtype=str),columns=[\"Index\",\n",
    "                                                            \"SegId\",\n",
    "                                                            \"NVoxels\",\n",
    "                                                            \"Volume_mm3\",\n",
    "                                                            \"StructName\",\n",
    "                                                            \"normMean\",\n",
    "                                                            \"normStdDev\",\n",
    "                                                            \"normMin\",\n",
    "                                                            \"normMax\",\n",
    "                                                            \"normRange\"])\n",
    "\n",
    "        df = df.append(extract_other_vals_from_aseg(aseg_file), ignore_index=True)\n",
    "        \n",
    "        aparc_columns = [\"StructName\", \"NumVert\", \"SurfArea\", \"GrayVol\",\n",
    "                         \"ThickAvg\", \"ThickStd\", \"MeanCurv\", \"GausCurv\",\n",
    "                         \"FoldInd\", \"CurvInd\"]\n",
    "        tmp_lh = pd.DataFrame(np.genfromtxt(lh_aparc,dtype=str),columns=aparc_columns)\n",
    "        tmp_lh[\"StructName\"] = \"lh_\"+tmp_lh[\"StructName\"]\n",
    "        tmp_rh = pd.DataFrame(np.genfromtxt(rh_aparc,dtype=str),columns=aparc_columns)\n",
    "        tmp_rh[\"StructName\"] = \"rh_\"+tmp_rh[\"StructName\"]\n",
    "\n",
    "        aseg_melt = pd.melt(df[[\"StructName\",\"Volume_mm3\"]], id_vars=[\"StructName\"])\n",
    "        aseg_melt.rename(columns={\"StructName\": \"name\"},inplace=True)\n",
    "        aseg_melt[\"value\"] = aseg_melt[\"value\"].astype(float)\n",
    "        \n",
    "        lh_aparc_melt = pd.melt(tmp_lh,id_vars=[\"StructName\"])\n",
    "        lh_aparc_melt[\"value\"] = lh_aparc_melt[\"value\"].astype(float)\n",
    "        lh_aparc_melt[\"name\"] = lh_aparc_melt[\"StructName\"]+ \"_\"+lh_aparc_melt[\"variable\"]\n",
    "        \n",
    "        rh_aparc_melt = pd.melt(tmp_rh, id_vars=[\"StructName\"])\n",
    "        rh_aparc_melt[\"value\"] = rh_aparc_melt[\"value\"].astype(float)\n",
    "        rh_aparc_melt[\"name\"] = rh_aparc_melt[\"StructName\"]+ \"_\"+rh_aparc_melt[\"variable\"]\n",
    "        \n",
    "        output = aseg_melt[[\"name\",\n",
    "                            \"value\"]].append(lh_aparc_melt[[\"name\",\n",
    "                                                            \"value\"]],\n",
    "                                             ignore_index=True).append(rh_aparc_melt[[\"name\",\n",
    "                                                                                      \"value\"]],\n",
    "                                                                       ignore_index=True)\n",
    "        outdict = output.to_dict(orient=\"records\")\n",
    "        final_dict = {}\n",
    "        for pair in outdict:\n",
    "            final_dict[pair[\"name\"]] = pair[\"value\"]\n",
    "        return final_dict\n",
    "    \n",
    "    output_dict = convert_stats_to_json(aseg_file, lh_aparc, rh_aparc)\n",
    "    return output_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates valid Mindcontrol entries that are saved as .json files. \n",
    "# They can be loaded into the Mindcontrol database later\n",
    "\n",
    "def create_mindcontrol_entries(mindcontrol_base_dir, output_dir, subject, stats):\n",
    "    import os\n",
    "    from nipype.utils.filemanip import save_json\n",
    "    \n",
    "    metric_split = {\"brainmask\": [\"eTIV\", \"CortexVol\", \"TotalGrayVol\"],\n",
    "                    \"wm\": [\"WM-hypointensities\", \n",
    "                           \"Right-WM-hypointensities\",\"Left-WM-hypointensities\"],\n",
    "                    \"aparcaseg\":[]}\n",
    "    \n",
    "    volumes = [\"brainmask.nii.gz\", \"wm.nii.gz\", \"aparc+aseg.nii.gz\", \"T1.nii.gz\"]\n",
    "    volumes_list = [os.path.join(output_dir, subject, volume) for volume in volumes]\n",
    "\n",
    "    all_entries = []\n",
    "    \n",
    "    for idx, entry_type in enumerate([\"brainmask\", \"wm\", \"aparcaseg\"]):\n",
    "        entry = {\"entry_type\":entry_type, \n",
    "                 \"subject_id\": subject, \n",
    "                 \"name\": subject}\n",
    "        base_img = os.path.relpath(volumes_list[-1], mindcontrol_base_dir)\n",
    "        overlay_img = os.path.relpath(volumes_list[idx], mindcontrol_base_dir)\n",
    "        entry[\"check_masks\"] = [base_img, overlay_img]\n",
    "        entry[\"metrics\"] = {}\n",
    "        for metric_name in metric_split[entry_type]:\n",
    "            entry[\"metrics\"][metric_name] = stats.pop(metric_name)\n",
    "        if not len(metric_split[entry_type]):\n",
    "            entry[\"metrics\"] = stats\n",
    "        all_entries.append(entry)\n",
    "    \n",
    "    output_json = os.path.abspath(\"mindcontrol_entries.json\")\n",
    "    save_json(output_json, all_entries)\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "\n",
    "Define the nipype nodes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_node = Node(IdentityInterface(fields=['subject_id',\"subjects_dir\",\n",
    "                                            \"mindcontrol_base_dir\", \"output_dir\"]), name='inputnode')\n",
    "input_node.iterables=(\"subject_id\", subjects)\n",
    "input_node.inputs.subjects_dir = subjects_dir\n",
    "input_node.inputs.mindcontrol_base_dir = mindcontrol_base_dir #this is where start_static_server is running\n",
    "input_node.inputs.output_dir = mindcontrol_outdir #this is in the freesurfer/ directory under the base_dir\n",
    "\n",
    "dg_node=Node(Function(input_names=[\"subjects_dir\", \"subject\", \"volumes\"],\n",
    "                      output_names=[\"volume_paths\"], \n",
    "                      function=data_grabber), \n",
    "             name=\"datagrab\")\n",
    "#dg_node.inputs.subjects_dir = subjects_dir\n",
    "dg_node.inputs.volumes = volumes\n",
    "\n",
    "\n",
    "mriconvert_node = MapNode(MRIConvert(out_type=\"niigz\"), \n",
    "                          iterfield=[\"in_file\"], \n",
    "                          name='convert')\n",
    "\n",
    "get_stats_node = Node(Function(input_names=[\"subjects_dir\", \"subject\"],\n",
    "                               output_names = [\"output_dict\"],\n",
    "                               function=parse_stats), name=\"get_freesurfer_stats\")\n",
    "\n",
    "write_mindcontrol_entries = Node(Function(input_names = [\"mindcontrol_base_dir\",\n",
    "                                                         \"output_dir\",\n",
    "                                                         \"subject\",\n",
    "                                                         \"stats\"],\n",
    "                                          output_names=[\"output_json\"],\n",
    "                                          function=create_mindcontrol_entries), \n",
    "                                 name=\"get_mindcontrol_entries\")\n",
    "\n",
    "datasink_node = Node(DataSink(),\n",
    "                     name='datasink')\n",
    "subst = [('out_file',''),('_subject_id_',''),('_out','')]  + [(\"_convert%d\" % index, \"\") for index in range(len(volumes))] \n",
    "datasink_node.inputs.substitutions = subst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Connect the nodes of the workflow and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(name=\"MindPrepFS\")\n",
    "wf.base_dir = workflow_working_dir\n",
    "wf.connect(input_node,\"subject_id\", dg_node,\"subject\")\n",
    "wf.connect(input_node,\"subjects_dir\", dg_node, \"subjects_dir\")\n",
    "wf.connect(input_node, \"subject_id\", get_stats_node, \"subject\")\n",
    "wf.connect(input_node, \"subjects_dir\", get_stats_node, \"subjects_dir\")\n",
    "wf.connect(input_node, \"subject_id\", write_mindcontrol_entries, \"subject\")\n",
    "wf.connect(input_node, \"mindcontrol_base_dir\", write_mindcontrol_entries, \"mindcontrol_base_dir\")\n",
    "wf.connect(input_node, \"output_dir\", write_mindcontrol_entries, \"output_dir\")\n",
    "wf.connect(get_stats_node, \"output_dict\", write_mindcontrol_entries, \"stats\")\n",
    "wf.connect(input_node, \"output_dir\", datasink_node, \"base_directory\")\n",
    "wf.connect(dg_node,\"volume_paths\", mriconvert_node, \"in_file\")\n",
    "wf.connect(mriconvert_node,'out_file',datasink_node,'out_file')\n",
    "wf.connect(write_mindcontrol_entries, \"output_json\", datasink_node, \"out_file.@json\")\n",
    "wf.write_graph(graph2use='exec')\n",
    "output = wf.run(plugin='MultiProc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the files in one of the subjects'd folders to make sure everything is there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(mindcontrol_outdir,subjects[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a graph of our workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(os.path.join(workflow_working_dir, 'MindPrepFS/graph_detailed.dot.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load JSON entries into the Mindcontrol database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection(port=7081):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient(\"localhost\", port)\n",
    "    db =  client.meteor\n",
    "    collection = db.subjects\n",
    "    return collection, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnude_metadata_pklz = Path('giedd_metadata.pklz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.read_pickle(dnude_metadata_pklz)\n",
    "df_metadata = df_metadata.assign(sub_sess = 'sub-' + df_metadata.bids_subj + '_ses-' + df_metadata.bids_ses)\n",
    "msk = df_metadata.dtypes == np.float  # or object, etc.\n",
    "df_metadata.loc[:, msk] = df_metadata.loc[:, msk].applymap(float)\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mriqc = df_metadata.loc[:,'cjv':'sub_sess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_manual = pd.concat(\n",
    "    [df_metadata.loc[:,\"ScanFormat\": 'Post_MNI_CIVET_QC_categorical'].drop(axis = 1, labels = ['Fam','FamID','MRN','LN','FN']),\n",
    "    df_metadata.loc[:,\"sub_sess\"]],\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell was run on a host with a running meteor instance and mongo server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a json file reader\n",
    "from nipype.utils.filemanip import load_json\n",
    "\n",
    "#get the mongo database collection\n",
    "coll, cli = get_collection(7081)\n",
    "\n",
    "#for subject's entry file, load the data and insert or update it into the database\n",
    "for entry_file in glob(os.path.join(mindcontrol_outdir,\"*\",\"mindcontrol_entries.json\")):\n",
    "    data = load_json(entry_file)\n",
    "    for d in data: \n",
    "        name = d['name']\n",
    "        \n",
    "        ## additionally add an entry for mriqc\n",
    "        if entry_type =='brainmask':\n",
    "            mriqc_entry = d\n",
    "            if '_id' in mriqc_entry.keys():\n",
    "                mriqc_entry.pop('_id')\n",
    "            mriqc_entry['entry_type']  = \"mriqc_entry\"\n",
    "            metrics = (df_mriqc.\n",
    "                                      query(\"sub_sess == @name\").\n",
    "                                      drop(axis = 1,\n",
    "                                          labels = ['sub_sess'])\n",
    "                                     )\n",
    "            if len(metrics) != 1:\n",
    "                print('skipping: ', entry_file)\n",
    "                break\n",
    "            mriqc_entry['metrics'] = metrics.iloc[0,:].to_dict()\n",
    "            query = {\"name\": mriqc_entry['name'], \"entry_type\": mriqc_entry['entry_type']}\n",
    "            res = coll.find_one(query)\n",
    "            #if the entry for a subject/entry_type does not exist, insert it:\n",
    "            if not res:\n",
    "                coll.insert_one(mriqc_entry)\n",
    "            #if the entry exists, simply update it\n",
    "            else:\n",
    "                coll.update_one(query, {\"$set\": mriqc_entry})\n",
    "\n",
    "        if entry_type =='brainmask':\n",
    "            manual_meta = d\n",
    "            if '_id' in manual_meta.keys():\n",
    "                manual_meta.pop('_id')\n",
    "            manual_meta['entry_type']  = \"manual_meta\"\n",
    "            metrics = (df_meta_manual.\n",
    "                                      query('sub_sess == @name').\n",
    "                                      drop(axis = 1,\n",
    "                                          labels = ['sub_sess'])\n",
    "                                     )\n",
    "            assert(len(metrics) == 1)\n",
    "            manual_meta['metrics'] = metrics.iloc[0,:].to_dict()\n",
    "            query = {\"name\": manual_meta['name'], \"entry_type\": manual_meta['entry_type']}\n",
    "            res = coll.find_one(query)\n",
    "            #if the entry for a subject/entry_type does not exist, insert it:\n",
    "            if not res:\n",
    "                coll.insert_one(manual_meta)\n",
    "            #if the entry exists, simply update it\n",
    "            else:\n",
    "                coll.update_one(query, {\"$set\": manual_meta})\n",
    "\n",
    "        # add original entries\n",
    "        name, entry_type = d[\"name\"], d[\"entry_type\"]\n",
    "\n",
    "        query = {\"name\": name, \"entry_type\": entry_type}\n",
    "        res = coll.find_one(query)\n",
    "        #if the entry for a subject/entry_type does not exist, insert it:\n",
    "        if not res:\n",
    "            coll.insert_one(d)\n",
    "        #if the entry exists, simply update it\n",
    "        else:\n",
    "            coll.update_one(query, {\"$set\": d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to http://localhost:3000, You should now see something like:\n",
    "    \n",
    "![](images/freesurfer_instance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying painter edits to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the QC viewer of a Brainmask file (click on an entry in the Freesurfer ID column), sign in, and click on the Painter pencil icon. I zoomed in (using alt+left click and drag) and erased an arbitrary chunk of voxels of the brainmask, and also logged a point at the center of my erased blob (will be used to plot later):\n",
    "\n",
    "![](images/paint_brainmask.png)\n",
    "\n",
    "Remember to hit \"Save\" when you are done. Now we will load the world coordinates that we painted and apply them to the brainmask file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from os.path import join, exists, split\n",
    "from dipy.tracking import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to load the nifti data using a database query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_mask(db, query,mindcontrol_base_dir):\n",
    "    cursor = db.find(query)\n",
    "    results = []\n",
    "    for item in cursor:\n",
    "        results.append(item)\n",
    "    \n",
    "    assert(len(results) == 1)\n",
    "    subject = results[0]\n",
    "\n",
    "    return nib.load(join(mindcontrol_base_dir,subject[\"check_masks\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts world coordinates to numpy matrix indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indices(streamline, aff, img):\n",
    "    \n",
    "    topoints = lambda x : np.array([[m[\"x\"], m[\"y\"], m[\"z\"]] for m in x[\"world_coor\"]])\n",
    "    points_orig = topoints(streamline)\n",
    "    points_nifti_space = list(utils.move_streamlines([points_orig], aff, input_space=aff))[0]\n",
    "    from dipy.tracking._utils import _to_voxel_coordinates, _mapping_to_voxel\n",
    "    lin_T, offset = _mapping_to_voxel(aff, None)\n",
    "    idx = _to_voxel_coordinates(points_orig, lin_T, offset)\n",
    "    return points_nifti_space, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the indices of points to paint and their new paint value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points_to_paint(drawing, aff, img): \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame()\n",
    "    data = img.get_data()\n",
    "    for d in drawing:\n",
    "        if not \"paintValue\" in d.keys():\n",
    "            continue\n",
    "        pv = d[\"paintValue\"]\n",
    "        if len(d[\"world_coor\"]):\n",
    "            points_nii_space, trans_points = convert_to_indices(d, aff, img)\n",
    "            tmp = []\n",
    "            for i,ni in enumerate(trans_points):\n",
    "                to_append = {\"x\": ni[0], \"y\":ni[1], \"z\": ni[2], \"val\": pv}\n",
    "\n",
    "                #validate affine. \n",
    "                if len(d[\"world_coor\"]) == len(d[\"matrix_coor\"]):\n",
    "                    old_val_client = d[\"matrix_coor\"][i][\"old_val\"]\n",
    "                    true_val = data[ni[0], ni[1], ni[2]]\n",
    "                    assert old_val_client == true_val, \"affine mismatch!\"\n",
    "\n",
    "                tmp.append(to_append)\n",
    "            df = df.append(pd.DataFrame(tmp), ignore_index=True)\n",
    "    if df.shape[0]:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final function to read the database for coordinates, load the image data, apply the painter edits, and save the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paint_volume(mongo_port, query,mindcontrol_base_dir, outpath):\n",
    "    coll, cli = get_collection(mongo_port)\n",
    "    mask_img = get_segmentation_mask(coll,query, mindcontrol_base_dir)\n",
    "    aff = mask_img.affine\n",
    "\n",
    "    p1 = get_points_to_paint(coll.find_one(query)[\"painters\"], aff, mask_img)\n",
    "\n",
    "    data = mask_img.get_data()\n",
    "    for idx, row in p1.iterrows():\n",
    "        paint_value = row['val']\n",
    "        x,y,z = row['x'], row['y'], row['z']\n",
    "        data[x][y][z] = paint_value\n",
    "\n",
    "    painted_image = nib.nifti1.Nifti1Image(data,aff,mask_img.header)\n",
    "    painted_image.to_filename(outpath)\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on sub-02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = create_paint_volume(3001, {\"subject_id\": \"sub-02\", \n",
    "                           \"entry_type\":\"brainmask\"},\n",
    "                    mindcontrol_base_dir,\n",
    "                   os.path.abspath(\"./mindcontrol_base_dir/freesurfer/sub-02/brainmask_edited.nii.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure this worked by plotting in nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = coll.find_one({\"subject_id\": \"sub-02\", \"entry_type\":\"brainmask\"})\n",
    "idx = entry[\"loggedPoints\"][0][\"world_coor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = subplots(1,1,figsize=(10,10))\n",
    "plot_roi(outpath, \"./mindcontrol_base_dir/freesurfer/sub-02/T1.nii.gz\", \n",
    "         cut_coords=[idx[\"y\"]], display_mode=\"y\", axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make BIDS dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids['fs_t1_path'] = (df_bids.\n",
    "                         apply(lambda df:\n",
    "                               (Path(mindcontrol_base_dir) / 'freesurfer' / ('sub-' + df.bids_subj + '_ses-' + df.bids_ses)  / 'T1.nii.gz') ,\n",
    "                               axis = 1)\n",
    "                        )\n",
    "df_bids.fs_t1_path.apply(Path.exists).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_symlink(row, overwrite_previous=False,symlink_path = 'symlink_path', source_path = 'dicom_path'):\n",
    "    \"\"\"\n",
    "    Create symlink to source path\n",
    "    \"\"\"\n",
    "    original_dir = Path.cwd()\n",
    "    \n",
    "    sympath = Path(row[symlink_path]).absolute().resolve()\n",
    "    source_path = Path(row[source_path]).absolute()\n",
    "    relative_path = os.path.relpath(source_path, sympath.parent)\n",
    "    print(sympath)\n",
    "    print(source_path)\n",
    "    print(relative_path)\n",
    "    symlinked = []\n",
    "    if sympath.exists() and not overwrite_previous:\n",
    "        symlinked = False\n",
    "        print('Symlink exists already, supply \"overwrite_previous\" argument\\\n",
    "         if you wish to overwrite it')\n",
    "    else:\n",
    "        if sympath.exists():\n",
    "            os.remove(sympath.absolute())\n",
    "        if not sympath.absolute().parent.exists():\n",
    "            os.makedirs(sympath.absolute().parent)\n",
    "        stdout = os.chdir(sympath.absolute().parent)\n",
    "#         print(\"symlink %s to %s with link %s\"% (sympath, source_path, os.path.relpath(source_path, sympath.parent)))\n",
    "        stdout = sympath.symlink_to(\n",
    "            relative_path)\n",
    "        symlinked = True\n",
    "    stdout = os.chdir(original_dir)\n",
    "    return symlinked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linked_bids_dir = Path('linked_bids_2018_08_14')\n",
    "if not linked_bids_dir.exists():\n",
    "    linked_bids_dir.mkdir()\n",
    "\n",
    "df_bids['bids_nifti_path'] = df_bids.apply(\n",
    "    lambda row: linked_bids_dir.absolute() / ('sub-' + row.bids_subj) / ('ses-' + row.bids_ses) / 'anat' / ('sub-' + row.bids_subj + '_ses-' + row.bids_ses + '_run-01_T1w.nii.gz'),\n",
    "    axis = 1)\n",
    "    \n",
    "import heudiconv_helpers as hh\n",
    "\n",
    "df_bids.apply(lambda row: make_symlink(row, overwrite_previous=True,symlink_path= 'bids_nifti_path', source_path = 'nifti_path'),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls linked_bids_2018_08_14/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bids.to_pickle(bids_info_pklz)\n",
    "df_bids.to_csv(bids_info_pklz.with_suffix('.psv'),index=False,sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
